{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a78fbb-68b9-4500-af63-543aeb21928b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../utils\")\n",
    "from logger import log_silver_ingestion\n",
    "import pyspark.sql.functions as F\n",
    "import uuid\n",
    "from delta.tables import DeltaTable\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ed6d43-8e1d-4947-b296-e3eb348b5334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setting Paths\n",
    "BRONZE_TABLE = \"nyc_taxi.bronze.yellow_taxi_trips\"\n",
    "SILVER_TABLE = \"nyc_taxi.silver.yellow_taxi_trips\"\n",
    "QUARANTINE_TABLE = \"nyc_taxi.quarantine.yellow_taxi_trips\"\n",
    "LOG_TABLE_SILVER= \"nyc_taxi.logs.silver_ingestion_logs\"\n",
    "dataset_name=\"yellow_taxi_trips\"\n",
    "run_id=str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c3b8850-6c53-42d9-bcad-6099e1cb48e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setting Types\n",
    "\n",
    "type_mapping = {\n",
    "    \"year\":\"int\",\n",
    "    \"month\":\"int\",\n",
    "    \"vendor_id\":\"int\",\n",
    "    \"pickup_datetime\":\"timestamp\",\n",
    "    \"dropoff_datetime\":\"timestamp\",\n",
    "    \"trip_distance\":\"double\",\n",
    "    \"passenger_count\":\"int\",\n",
    "    \"rate_code_id\":\"int\",\n",
    "    \"pickup_location_id\":\"int\",\n",
    "    \"dropoff_location_id\":\"int\",\n",
    "    \"payment_type_id\":\"int\",\n",
    "    \"fare_amount\":\"double\",\n",
    "    \"extra_charge\":\"double\",\n",
    "    \"mta_tax\":\"double\",\n",
    "    \"tip_amount\":\"double\",\n",
    "    \"tolls_amount\":\"double\",\n",
    "    \"improvement_surcharge\":\"double\",\n",
    "    \"congestion_surcharge\":\"double\",\n",
    "    \"cbd_congestion_fee\":\"double\",\n",
    "    \"airport_fee\":\"double\",\n",
    "    \"total_amount\":\"double\",\n",
    "    \"store_and_fwd_flag\":\"boolean\",\n",
    "    \"bronze_id\":\"string\"\n",
    "}\n",
    "\n",
    "# Setting Columns to build hash\n",
    "\n",
    "hash_columns = [\"vendor_id\",\"pickup_datetime\",\"dropoff_datetime\",\n",
    "                \"rate_code_id\",\"pickup_location_id\",\"dropoff_location_id\",\n",
    "                \"payment_type_id\"]\n",
    "\n",
    "# Validation rules for silver table\n",
    "silver_rules = [\n",
    "    F.when(F.col(\"vendor_id\").isNull(), F.lit(\"vendor_id_null\")),\n",
    "    F.when(F.col(\"passenger_count\").isNull(), F.lit(\"passenger_count_null\")),\n",
    "    F.when(~F.col(\"passenger_count\").between(1,6), F.lit(\"invalid_passenger_count\")),\n",
    "    F.when(F.col(\"trip_distance\").isNull(), F.lit(\"trip_distance_null\")),\n",
    "    F.when(F.col(\"trip_distance\") < 0, F.lit(\"invalid_trip_distance\")),\n",
    "    F.when(F.col(\"store_and_fwd_flag\").isNull(), F.lit(\"store_and_fwd_flag_null\")),\n",
    "    F.when(F.col(\"rate_code_id\").isNull(), F.lit(\"rate_code_id_null\")),\n",
    "    F.when(F.col(\"pickup_location_id\").isNull() | F.col(\"dropoff_location_id\").isNull(), F.lit(\"pickup_dropoff_location_null\")),\n",
    "    F.when(F.col(\"pickup_datetime\").isNull(), F.lit(\"pickup_datetime_null\")),\n",
    "    F.when(F.col(\"dropoff_datetime\").isNull(), F.lit(\"dropoff_datetime_null\")),\n",
    "    F.when(F.col(\"pickup_datetime\") > F.col(\"dropoff_datetime\"), F.lit(\"pickup_datetime_after_dropoff\")),\n",
    "    F.when(F.col(\"payment_type_id\").isNull(), F.lit(\"payment_type_id_null\")),\n",
    "    F.when(F.col(\"fare_amount\").isNull(), F.lit(\"fare_amount_null\")),\n",
    "    F.when(F.col(\"fare_amount\") < 0, F.lit(\"invalid_fare_amount\")),\n",
    "    F.when(F.col(\"extra_charge\").isNull(), F.lit(\"extra_charge_null\")),\n",
    "    F.when(F.col(\"extra_charge\")<0, F.lit(\"extra_charge_invalid\")),\n",
    "    F.when(F.col(\"mta_tax\").isNull(), F.lit(\"mta_tax_null\")),\n",
    "    F.when(~F.col(\"mta_tax\").isin(0,0.5), F.lit(\"mta_tax_invalid\")),\n",
    "    F.when(F.col(\"tip_amount\").isNull(), F.lit(\"tip_amount_null\")),\n",
    "    F.when(F.col(\"tip_amount\")<0, F.lit(\"tip_amount_null\")),  \n",
    "    F.when(F.col(\"tolls_amount\").isNull(), F.lit(\"tolls_amount_null\")),\n",
    "    F.when(F.col(\"tolls_amount\")<0, F.lit(\"tolls_amount_invalid\")),\n",
    "    F.when(F.col(\"improvement_surcharge\").isNull(), F.lit(\"improvement_surcharge_null\")),\n",
    "    F.when(~F.col(\"improvement_surcharge\").isin(0,0.3,1), F.lit(\"improvement_surcharge_invalid\")),\n",
    "    F.when(F.col(\"congestion_surcharge\").isNull(), F.lit(\"congestion_surcharge_null\")),\n",
    "    F.when(~F.col(\"congestion_surcharge\").isin(0,2.5), F.lit(\"congestion_surcharge_invalid\")),\n",
    "    F.when(F.col(\"airport_fee\").isNull(), F.lit(\"airport_fee_null\")),\n",
    "    F.when(~F.col(\"airport_fee\").isin(0,1.75), F.lit(\"airport_fee_invalid\")),\n",
    "    F.when(F.col(\"trip_duration\").isNull(), F.lit(\"trip_duration_null\")),\n",
    "    F.when(F.col(\"trip_duration\") < 0, F.lit(\"invalid_trip_duration\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baaba662-6673-4942-af95-1f8e97cf55ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- INITIALIZATION & CONFIGURATION ---\n",
    "start_ts = datetime.datetime.now()\n",
    "\n",
    "# Initialize metrics & state flags\n",
    "bronze_df = None\n",
    "silver_count = quarantine_count = duplicates_dropped = bronze_count = 0\n",
    "silver_survivors = quarantine_survivors = 0\n",
    "success = silver_load_success = quarantine_load_success = False\n",
    "error_msg = \"\"\n",
    "init_run = 0 # Set to 1 to skip watermark retrieval for first-time setup\n",
    "look_back_minutes = 5\n",
    "\n",
    "# Ensure Logging Table exists\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {LOG_TABLE_SILVER} USING DELTA\")\n",
    "\n",
    "try:\n",
    "    # 1. WATERMARKING: Incremental Data Retrieval\n",
    "    if init_run == 0:\n",
    "        print(\"Retrieving last ingested timestamp...\")\n",
    "        log_history = spark.read.table(LOG_TABLE_SILVER).filter(f\"status = 'Success' AND dataset_name = '{dataset_name}'\")\n",
    "        last_ingested_ts = log_history.agg({\"max_bronze_ts\": \"max\"}).collect()[0][0] if log_history.count() > 0 else None\n",
    "    else:\n",
    "        last_ingested_ts = None\n",
    "\n",
    "    # 2. INGESTION: Read with Lookback for late-arriving data\n",
    "    print(\"Ingesting bronze data...\")\n",
    "    if last_ingested_ts:\n",
    "        read_watermark = last_ingested_ts - datetime.timedelta(minutes=look_back_minutes)\n",
    "        bronze_df = spark.read.table(BRONZE_TABLE).filter(f\"_ingest_ts > timestamp('{read_watermark}')\")\n",
    "    else:\n",
    "        bronze_df = spark.read.table(BRONZE_TABLE)\n",
    "\n",
    "    bronze_count = bronze_df.count()\n",
    "\n",
    "    if bronze_count > 0:\n",
    "        print(f\"Processing {bronze_count} records...\")\n",
    "\n",
    "        # 3. TRANSFORMATIONS: Schema Standardization & Feature Engineering\n",
    "        # Rename columns to snake_case and cast types via predefined 'type_mapping'\n",
    "        df_transformed = (bronze_df\n",
    "            .withColumnsRenamed({\n",
    "                \"tpep_pickup_datetime\": \"pickup_datetime\", \"tpep_dropoff_datetime\": \"dropoff_datetime\",\n",
    "                \"VendorID\": \"vendor_id\", \"RatecodeID\": \"rate_code_id\",\n",
    "                \"PULocationID\": \"pickup_location_id\", \"DOLocationID\": \"dropoff_location_id\",\n",
    "                \"payment_type\": \"payment_type_id\", \"extra\": \"extra_charge\",\n",
    "                \"Airport_fee\": \"airport_fee\", \"run_id\": \"bronze_id\"\n",
    "            })\n",
    "            .withColumn(\"store_and_fwd_flag\", F.col(\"store_and_fwd_flag\") == \"Y\")\n",
    "            .select([F.col(c).cast(t) for c, t in type_mapping.items()])\n",
    "            .withColumn(\"trip_id\", F.sha2(F.concat_ws(\"||\", *hash_columns), 256))\n",
    "            .withColumn(\"driver_pay\", (F.col(\"fare_amount\") + F.col(\"tip_amount\")).cast(\"double\"))\n",
    "            .withColumn(\"trip_duration\", (F.col('dropoff_datetime').cast(\"long\") - F.col('pickup_datetime').cast(\"long\")).cast(\"int\"))\n",
    "            .withColumn(\"ingest_ts\", F.current_timestamp())\n",
    "            .withColumn(\"run_id\", F.lit(run_id))\n",
    "            .dropDuplicates([\"trip_id\"]))\n",
    "\n",
    "        # 4. VALIDATION: Apply DQ Rules and identify Quarantine reasons\n",
    "        df_validated = df_transformed.withColumn(\"quarantine_reasons\", F.array())\n",
    "        for rule in silver_rules:\n",
    "            df_validated = df_validated.withColumn(\n",
    "                \"quarantine_reasons\",\n",
    "                F.when(rule.isNotNull(), F.array_union(\"quarantine_reasons\", F.array(rule))).otherwise(F.col(\"quarantine_reasons\"))\n",
    "            )\n",
    "\n",
    "        # 5. SPLIT: Divide data into Silver (Clean) and Quarantine (Dirty)\n",
    "        silver_df = df_validated.filter(F.size(\"quarantine_reasons\") == 0).drop(\"quarantine_reasons\")\n",
    "        quarantine_df = df_validated.filter(F.size(\"quarantine_reasons\") > 0)\n",
    "\n",
    "        # 6. ATOMIC WRITES: Idempotent Merge into Target Tables\n",
    "        # Write Silver\n",
    "        print(\"Merging Silver data...\")\n",
    "        s_table = DeltaTable.forName(spark, SILVER_TABLE)\n",
    "        s_table.alias(\"t\").merge(silver_df.alias(\"s\"), \"t.trip_id = s.trip_id\").whenNotMatchedInsertAll().execute()\n",
    "        silver_load_success = True\n",
    "        \n",
    "        # Write Quarantine\n",
    "        print(\"Merging Quarantine data...\")\n",
    "        q_table = DeltaTable.forName(spark, QUARANTINE_TABLE)\n",
    "        q_table.alias(\"t\").merge(quarantine_df.alias(\"s\"), \"t.trip_id = s.trip_id\").whenNotMatchedInsertAll().execute()\n",
    "        quarantine_load_success = True\n",
    "\n",
    "        # 7. METRICS: Efficient Audit using Delta Transaction Logs\n",
    "        s_metrics = s_table.history(1).select(\"operationMetrics\").collect()[0][0]\n",
    "        q_metrics = q_table.history(1).select(\"operationMetrics\").collect()[0][0]\n",
    "        \n",
    "        silver_count = int(s_metrics.get(\"numTargetRowsInserted\", 0))\n",
    "        quarantine_count = int(q_metrics.get(\"numTargetRowsInserted\", 0))\n",
    "        \n",
    "        # Calculate duplicates by comparing Bronze Input vs records sent to Merge\n",
    "        silver_survivors = int(s_metrics.get(\"numSourceRows\", 0))\n",
    "        quarantine_survivors = int(q_metrics.get(\"numSourceRows\", 0))\n",
    "        duplicates_dropped = bronze_count - (silver_survivors + quarantine_survivors)\n",
    "\n",
    "        max_bronze_ts = df_validated.select(F.max(\"ingest_ts\")).collect()[0][0]\n",
    "        success = True\n",
    "        print(f\"Job Successful. Silver: {silver_count}, Quarantine: {quarantine_count}, Dropped: {duplicates_dropped}\")\n",
    "\n",
    "    else:\n",
    "        max_bronze_ts = last_ingested_ts\n",
    "        error_msg = \"No new data to process\"\n",
    "        success=True\n",
    "        print(error_msg)\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    print(f\"Job Failed: {error_msg}\")\n",
    "    \n",
    "    # Automatic Rollback using Delta Time Travel\n",
    "    for table, flag in [(SILVER_TABLE, silver_load_success), (QUARANTINE_TABLE, quarantine_load_success)]:\n",
    "        if flag:\n",
    "            print(f\"Rolling back {table}...\")\n",
    "            history = spark.sql(f\"DESCRIBE HISTORY {table}\").collect()\n",
    "            if len(history) > 1:\n",
    "                prev_v = history[0][0] - 1\n",
    "                spark.sql(f\"RESTORE TABLE {table} TO VERSION AS OF {prev_v}\")\n",
    "\n",
    "finally:\n",
    "    # Record audit log\n",
    "    end_ts = datetime.datetime.now()\n",
    "    \n",
    "    log_silver_ingestion(\n",
    "        spark=spark, run_id=run_id, dataset_name=dataset_name, start_ts=start_ts, \n",
    "        end_ts=end_ts, max_bronze_ts=max_bronze_ts, success=success, \n",
    "        bronze_count=bronze_count, silver_count=silver_count, \n",
    "        quarantine_count=quarantine_count, duplicates_dropped=duplicates_dropped,\n",
    "        error_msg=error_msg, log_table=LOG_TABLE_SILVER, catalog_table=SILVER_TABLE\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_ingest_yellow_taxi",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
