{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6a78fbb-68b9-4500-af63-543aeb21928b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../utils\")\n",
    "from logger import log_silver_ingestion\n",
    "import pyspark.sql.functions as F\n",
    "import uuid\n",
    "from delta.tables import DeltaTable\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ed6d43-8e1d-4947-b296-e3eb348b5334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setting Paths\n",
    "BRONZE_TABLE = \"nyc_taxi.bronze.yellow_taxi_trips\"\n",
    "SILVER_TABLE = \"nyc_taxi.silver.yellow_taxi_trips\"\n",
    "QUARANTINE_TABLE = \"nyc_taxi.quarantine.yellow_taxi_trips\"\n",
    "LOG_TABLE_SILVER= \"nyc_taxi.logs.silver_ingestion_logs\"\n",
    "dataset_name=\"yellow_taxi_trips\"\n",
    "run_id=str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c3b8850-6c53-42d9-bcad-6099e1cb48e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setting Types\n",
    "\n",
    "type_mapping = {\n",
    "    \"year\":\"int\",\n",
    "    \"month\":\"int\",\n",
    "    \"vendor_id\":\"int\",\n",
    "    \"pickup_datetime\":\"timestamp\",\n",
    "    \"dropoff_datetime\":\"timestamp\",\n",
    "    \"trip_distance\":\"double\",\n",
    "    \"passenger_count\":\"int\",\n",
    "    \"rate_code_id\":\"int\",\n",
    "    \"pickup_location_id\":\"int\",\n",
    "    \"dropoff_location_id\":\"int\",\n",
    "    \"payment_type_id\":\"int\",\n",
    "    \"fare_amount\":\"double\",\n",
    "    \"extra_charge\":\"double\",\n",
    "    \"mta_tax\":\"double\",\n",
    "    \"tip_amount\":\"double\",\n",
    "    \"tolls_amount\":\"double\",\n",
    "    \"improvement_surcharge\":\"double\",\n",
    "    \"congestion_surcharge\":\"double\",\n",
    "    \"cbd_congestion_fee\":\"double\",\n",
    "    \"airport_fee\":\"double\",\n",
    "    \"total_amount\":\"double\",\n",
    "    \"store_and_fwd_flag\":\"boolean\",\n",
    "    \"bronze_id\":\"string\"\n",
    "}\n",
    "\n",
    "# Setting Columns to build hash\n",
    "\n",
    "hash_columns = [\"vendor_id\",\"pickup_datetime\",\"dropoff_datetime\",\n",
    "                \"rate_code_id\",\"pickup_location_id\",\"dropoff_location_id\",\n",
    "                \"payment_type_id\"]\n",
    "\n",
    "# Validation rules for silver table\n",
    "silver_rules = [\n",
    "    F.when(F.col(\"vendor_id\").isNull(), F.lit(\"vendor_id_null\")),\n",
    "    F.when(F.col(\"passenger_count\").isNull(), F.lit(\"passenger_count_null\")),\n",
    "    F.when(~F.col(\"passenger_count\").between(1,6), F.lit(\"invalid_passenger_count\")),\n",
    "    F.when(F.col(\"trip_distance\").isNull(), F.lit(\"trip_distance_null\")),\n",
    "    F.when(F.col(\"trip_distance\") < 0, F.lit(\"invalid_trip_distance\")),\n",
    "    F.when(F.col(\"store_and_fwd_flag\").isNull(), F.lit(\"store_and_fwd_flag_null\")),\n",
    "    F.when(F.col(\"rate_code_id\").isNull(), F.lit(\"rate_code_id_null\")),\n",
    "    F.when(F.col(\"pickup_location_id\").isNull() | F.col(\"dropoff_location_id\").isNull(), F.lit(\"pickup_dropoff_location_null\")),\n",
    "    F.when(F.col(\"pickup_datetime\").isNull(), F.lit(\"pickup_datetime_null\")),\n",
    "    F.when(F.col(\"dropoff_datetime\").isNull(), F.lit(\"dropoff_datetime_null\")),\n",
    "    F.when(F.col(\"pickup_datetime\") > F.col(\"dropoff_datetime\"), F.lit(\"pickup_datetime_after_dropoff\")),\n",
    "    F.when(F.col(\"payment_type_id\").isNull(), F.lit(\"payment_type_id_null\")),\n",
    "    F.when(F.col(\"fare_amount\").isNull(), F.lit(\"fare_amount_null\")),\n",
    "    F.when(F.col(\"fare_amount\") < 0, F.lit(\"invalid_fare_amount\")),\n",
    "    F.when(F.col(\"extra_charge\").isNull(), F.lit(\"extra_charge_null\")),\n",
    "    F.when(F.col(\"extra_charge\")<0, F.lit(\"extra_charge_invalid\")),\n",
    "    F.when(F.col(\"mta_tax\").isNull(), F.lit(\"mta_tax_null\")),\n",
    "    F.when(~F.col(\"mta_tax\").isin(0,0.5), F.lit(\"mta_tax_invalid\")),\n",
    "    F.when(F.col(\"tip_amount\").isNull(), F.lit(\"tip_amount_null\")),\n",
    "    F.when(F.col(\"tip_amount\")<0, F.lit(\"tip_amount_null\")),  \n",
    "    F.when(F.col(\"tolls_amount\").isNull(), F.lit(\"tolls_amount_null\")),\n",
    "    F.when(F.col(\"tolls_amount\")<0, F.lit(\"tolls_amount_invalid\")),\n",
    "    F.when(F.col(\"improvement_surcharge\").isNull(), F.lit(\"improvement_surcharge_null\")),\n",
    "    F.when(~F.col(\"improvement_surcharge\").isin(0,0.3), F.lit(\"improvement_surcharge_invalid\")),\n",
    "    F.when(F.col(\"congestion_surcharge\").isNull(), F.lit(\"congestion_surcharge_null\")),\n",
    "    F.when(~F.col(\"congestion_surcharge\").isin(0,2.5), F.lit(\"congestion_surcharge_invalid\")),\n",
    "    F.when(F.col(\"airport_fee\").isNull(), F.lit(\"airport_fee_null\")),\n",
    "    F.when(~F.col(\"airport_fee\").isin(0,1.75), F.lit(\"airport_fee_invalid\")),\n",
    "    F.when(F.col(\"trip_duration\").isNull(), F.lit(\"trip_duration_null\")),\n",
    "    F.when(F.col(\"trip_duration\") < 0, F.lit(\"invalid_trip_duration\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baaba662-6673-4942-af95-1f8e97cf55ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logging job start\n",
    "start_ts = datetime.datetime.now()\n",
    "# Initialization   \n",
    "bronze_df = None\n",
    "silver_df = None\n",
    "quarantine_df = None\n",
    "max_bronze_ts = None\n",
    "last_ingested_ts = None\n",
    "success = False\n",
    "error_msg = \"\"\n",
    "silver_load_success= False\n",
    "quarantine_load_success= False\n",
    "# Set for initial run indication\n",
    "init_run=0\n",
    "# Setting lookback window\n",
    "look_back_minutes = 5\n",
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {LOG_TABLE_SILVER} USING DELTA\")\n",
    "\n",
    "try:\n",
    "    #only retrieve for non-initial run\n",
    "    if init_run ==0:\n",
    "\n",
    "    # Retrieve last successful silver run timestamp\n",
    "        print(\"Retrieving last ingested timestamp...\")\n",
    "        log_history = spark.read.table(LOG_TABLE_SILVER).filter(f\"status = 'Success' AND dataset_name = '{dataset_name}'\")\n",
    "        last_ingested_ts = log_history.agg({\"max_bronze_ts\": \"max\"}).collect()[0][0] if log_history.count() > 0 else None\n",
    "\n",
    "    # Ingest Bronze data incrementally\n",
    "        print(\"Ingesting bronze data...\")\n",
    "    if last_ingested_ts is not None:\n",
    "        read_watermark_ts = last_ingested_ts - datetime.timedelta(minutes=look_back_minutes)\n",
    "        bronze_df = spark.read.table(BRONZE_TABLE).filter(f\"ingest_ts > timestamp('{read_watermark_ts}')\")\n",
    "    else:\n",
    "        bronze_df = spark.read.table(BRONZE_TABLE)  # first run, ingest all\n",
    "\n",
    "    bronze_count=bronze_df.count()\n",
    "\n",
    "    if bronze_count > 0:\n",
    "        print(f\"Ingested {bronze_count} records.\")\n",
    "    # Standardizing Column Names and casting data types\n",
    "        df_renamed = bronze_df.withColumnsRenamed(\n",
    "            {\n",
    "                \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
    "                \"tpep_dropoff_datetime\": \"dropoff_datetime\",\n",
    "                \"VendorID\":\"vendor_id\",\n",
    "                \"RatecodeID\":\"rate_code_id\",\n",
    "                \"PULocationID\":\"pickup_location_id\",\n",
    "                \"DOLocationID\":\"dropoff_location_id\",\n",
    "                \"payment_type\":\"payment_type_id\",\n",
    "                \"extra\":\"extra_charge\",\n",
    "                \"Airport_fee\":\"airport_fee\",\n",
    "                \"run_id\":\"bronze_id\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        df_renamed= df_renamed.withColumn(\"store_and_fwd_flag\",F.when(F.col(\"store_and_fwd_flag\")==\"Y\",True).otherwise(False))\n",
    "        df_cast= df_renamed.select([F.col(col).cast(dtype) for col,dtype in type_mapping.items()])\n",
    "\n",
    "        # Transforming Data\n",
    "        df_with_hash = df_cast.withColumn(\"trip_id\",F.sha2(F.concat_ws(\"||\", *hash_columns), 256))\n",
    "        df_with_pay=df_with_hash.withColumn(\"driver_pay\",(F.col(\"fare_amount\")+F.col(\"tip_amount\")).cast(\"double\"))\n",
    "        df_with_duration = df_with_pay.withColumn(\"trip_duration\",((F.col('dropoff_datetime').cast(\"long\")-F.col('pickup_datetime').cast(\"long\")).cast(\"int\")))\n",
    "        #Metadata inclusion\n",
    "        df_with_metadata=df_with_duration.withColumn(\"ingest_ts\",F.current_timestamp()).withColumn(\"run_id\",F.lit(run_id))\n",
    "\n",
    "        # Deduplication\n",
    "        df_deduped=df_with_metadata.dropDuplicates([\"trip_id\"])\n",
    "\n",
    "        # Validating business and data quality rules\n",
    "        df_validated = df_deduped.withColumn(\"quarantine_reasons\", F.array())\n",
    "        for rule in silver_rules:\n",
    "            df_validated = df_validated.withColumn(\n",
    "                \"quarantine_reasons\",\n",
    "                F.when(rule.isNotNull(), F.array_union(F.col(\"quarantine_reasons\"), F.array(rule)))\n",
    "                .otherwise(F.col(\"quarantine_reasons\"))\n",
    "            )\n",
    "\n",
    "        # Splitting Data into Silver and Quarantine Tables\n",
    "        silver_df = df_validated.filter(F.size(F.col(\"quarantine_reasons\")) == 0).drop(\"quarantine_reasons\")\n",
    "        quarantine_df = df_validated.filter(F.size(F.col(\"quarantine_reasons\")) > 0)\n",
    "\n",
    "\n",
    "\n",
    "        # Writing silver data via merge, inserting new rows only\n",
    "        print(\"Writing to silver table...\")\n",
    "        silver_delta = DeltaTable.forName(spark, SILVER_TABLE)\n",
    "        silver_delta.alias(\"t\").merge(\n",
    "            silver_df.alias(\"s\"),\n",
    "            \"t.trip_id = s.trip_id\",\n",
    "            )\\\n",
    "            .whenNotMatchedInsertAll().execute()\n",
    "\n",
    "        silver_load_success=True\n",
    "        silver_count = silver_df.count()\n",
    "\n",
    "        print(\"Silver write successful.\")\n",
    "        print(f\"Inserted {silver_count} records into silver table.\")\n",
    "        # Writing Quarantine Data\n",
    "        print(\"Writing to quarantine table...\")\n",
    "        spark.sql(f\"CREATE TABLE IF NOT EXISTS {QUARANTINE_TABLE} USING DELTA\")\n",
    "        quarantine_delta = DeltaTable.forName(spark, QUARANTINE_TABLE)\n",
    "        quarantine_delta.alias(\"t\").merge(\n",
    "            quarantine_df.alias(\"s\"),\n",
    "            \"t.trip_id = s.trip_id\"\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "        quarantine_count = quarantine_df.count()\n",
    "        quarantine_load_success=True\n",
    "        print(\"Quarantine write successful.\")\n",
    "        print(f\"Inserted {quarantine_count} records into quarantine table.\")    \n",
    "\n",
    "        #Logging end of successful job run\n",
    "        max_bronze_ts= df_validated.select(F.max(\"ingest_ts\")).collect()[0][0]\n",
    "        success = True\n",
    "        error_msg = \"\"\n",
    "    else:\n",
    "        silver_count=0\n",
    "        quarantine_count=0\n",
    "        max_bronze_ts=last_ingested_ts\n",
    "        error_msg = \"No new data to process\"\n",
    "        print(error_msg)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Job Failed\\nError:{e}\")\n",
    "    success = False\n",
    "    error_msg = str(e)\n",
    "    max_bronze_ts=last_ingested_ts\n",
    "    if silver_load_success:\n",
    "        print(\"Rolling back Silver table...\")\n",
    "        # Get the version before the one we just created\n",
    "        prev_version = spark.sql(f\"DESCRIBE HISTORY {SILVER_TABLE}\").select(\"version\").first()[0] - 1\n",
    "        spark.sql(f\"RESTORE TABLE {SILVER_TABLE} TO VERSION AS OF {prev_version}\")\n",
    "        silver_load_success = False # Reset flag after rollback\n",
    "        silver_count = 0\n",
    "        print(f\"Silver rollback to version {prev_version} successful.\")\n",
    "    if quarantine_load_success:\n",
    "            print(\"Rolling back Quarantine table...\")\n",
    "            prev_version = spark.sql(f\"DESCRIBE HISTORY {QUARANTINE_TABLE}\").select(\"version\").first()[0] - 1\n",
    "            spark.sql(f\"RESTORE TABLE {QUARANTINE_TABLE} TO VERSION AS OF {prev_version}\")\n",
    "            quarantine_load_success = False\n",
    "            quarantine_count = 0 \n",
    "            print(f\"Quarantine rollback to version {prev_version} successful.\")\n",
    "    if not silver_load_success and not quarantine_load_success:\n",
    "        print(\"No data written to tables.\")       \n",
    "\n",
    "finally:\n",
    "    end_ts=datetime.datetime.now()\n",
    "    print(f\"Job completed in {end_ts-start_ts}\")\n",
    "    print(f\"Logging job run to {LOG_TABLE_SILVER}...\")\n",
    "    log_silver_ingestion(\n",
    "        spark=spark,\n",
    "        run_id=run_id,\n",
    "        dataset_name=dataset_name,\n",
    "        start_ts=start_ts,\n",
    "        end_ts=end_ts,\n",
    "        max_bronze_ts=max_bronze_ts,\n",
    "        success=success,\n",
    "        bronze_count=bronze_count if 'bronze_count' in locals() else 0,\n",
    "        silver_count=silver_count,\n",
    "        quarantine_count=quarantine_count,\n",
    "        error_msg=error_msg,\n",
    "        log_table = LOG_TABLE_SILVER,\n",
    "        catalog_table= SILVER_TABLE\n",
    "    )\n",
    "    print(\"Log entry completed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_ingest_yellow_taxi",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
